<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Mohd Mujtaba Akhtar — Portfolio</title>
  <link rel="stylesheet" href="style.css"/>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>
</head>
<body>
  <header>
    <div class="container header-flex">
      <div class="social">
        <a href="mailto:mmakhtar.research@gmail.com" title="Email"><i class="fas fa-envelope"></i></a>
        <a href="https://github.com/Akhtar1111352" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
        <a href="https://www.linkedin.com/in/mo-mujtaba-akhtar-b6bb77201/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
        <a href="https://scholar.google.com/citations?user=bwHU6i8AAAAJ" target="_blank" title="Scholar"><i class="fas fa-graduation-cap"></i></a>
      </div>
      <nav>
        <a href="#" class="nav-link active" data-section="about">about</a>
        <a href="#" class="nav-link" data-section="publications">publications</a>
        <a href="#" class="nav-link" data-section="news">news</a>
      </nav>
      <button id="toggle-theme" title="Toggle dark mode"><i class="fas fa-moon"></i></button>
    </div>
  </header>

  <main class="container">
    <!-- ABOUT -->
    <section class="profile-section spa-section" id="about">
      <div class="profile-left">
        <h1><span class="about-bold">Mohd Mujtaba</span> Akhtar</h1>
        <div class="about-pill-row">
<!--           <span class="about-pill">M.Tech (CS) @ VBSPU</span>
          <span class="about-pill"><a href="mailto:mmakhtar.research@gmail.com">mmakhtar.research@gmail.com</a></span> -->
        </div>
        <p>
          Hello! I am <span class="about-bold">Mohd Mujtaba Akhtar</span>. My core research interests revolve around <span class="about-highlight">speech and audio processing</span>, with a particular focus on <span class="about-highlight">audio deepfake detection</span>, <span class="about-highlight">emotional speech understanding</span>, and the application of <span class="about-highlight">multimodal and foundation models</span> in both behavioral and forensic domains.
        </p>
        <p>
          I am currently working on <span class="about-highlight">audio deepfake detection using transfer learning from foundation models</span> as part of my thesis work. 
          <!-- Previously, I have interned at organizations including <span class="about-highlight">Reliance Jio AICoE</span>, <span class="about-highlight">Artviewings</span>, <span class="about-highlight">Suratec Co. Ltd.</span>, and <span class="about-highlight">IBM</span>, gaining experience in <span class="about-highlight">multilingual speech analysis</span>, <span class="about-highlight">non-verbal emotion recognition</span>, and <span class="about-highlight">real-world machine learning applications</span>. -->
        </p>
        <p>
          I am actively seeking research collaboration opportunities in areas such as <span class="about-highlight">speech/audio deepfake detection</span>, <span class="about-highlight">audio-visual deepfake detection</span>, <span class="about-highlight">speech emotion recognition</span>, <span class="about-highlight">affective computing</span>, and <span class="about-highlight">speech-driven healthcare applications</span>. I am always open to new ideas and interdisciplinary projects—if you are interested in collaborating or have suitable opportunities (including research positions or internships), please feel free to reach out at : <a href="mailto:mmakhtar.research@gmail.com">mmakhtar.research@gmail.com</a>.
        </p>
        <p>
          I am passionate about pushing the boundaries of what is possible with <span class="about-highlight">speech and audio technology</span>. I look forward to connecting and working together!
        </p>
        <h2 class="about-h2">Research Interests</h2>
        <ul class="about-list">
          <li>Computational Speech Analysis</li>
          <li>Deepfake Forensics</li>
          <li>Emotional Speech Understanding</li>
          <li>Healthcare Applications (Speech)</li>
          <li>Multimodal/Foundation Models</li>
        </ul>
      </div>
      <div class="profile-right about-right">
        <img src="images/1.jpg" alt="Profile Photo">
        <!-- <div class="about-address">
          8013, Gates Hillman Center <br>
          Carnegie Mellon University<br>
          Pittsburgh, PA, 15217
        </div> -->
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section class="section spa-section" id="publications" style="display:none">
      <h2>Conference Publications</h2>
      <ul>
        <li>
          <b>PARROT: Synergizing Mamba and Attention-based SSL Pre-Trained Models via Parallel Branch Hadamard Optimal Transport for Speech Emotion Recognition</b><br>
          Orchid Chetia Phukan*, <b>Mohd Mujtaba Akhtar*</b>, Girish*, Swarup Ranjan Behera, Sai Kiran Patibandla, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.01138" target="_blank">Paper</a>
        </li>
        <li>
          <b>SNIFR: Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer</b><br>
          Orchid Chetia Phukan, <b>Mohd Mujtaba Akhtar*</b>, Girish*, Swarup Ranjan Behera, Abu Osama, Sarthak Jain, Priyabrata Mallick, Sai Kiran Patibandla, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.03378" target="_blank">Paper</a>
        </li>
        <li>
          <b>HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition</b><br>
          Orchid Chetia Phukan*, Girish*, <b>Mohd Mujtaba Akhtar*</b>, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.03403" target="_blank">Paper</a>
        </li>
        <li>
          <b>Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction</b><br>
          Orchid Chetia Phukan*, Girish*, <b>Mohd Mujtaba Akhtar*</b>, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.02232" target="_blank">Paper</a>
        </li>
        <li>
          <b>Towards Machine Unlearning for Paralinguistic Speech Processing</b><br>
          Orchid Chetia Phukan*, Girish*, <b>Mohd Mujtaba Akhtar*</b>, Shubham Singh, Swarup Ranjan Behera, Vandana Rajan, Muskaan Singh, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.02230" target="_blank">Paper</a>
        </li>
        
        
        <li>
          <b>Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism</b><br>
          Orchid Chetia Phukan*, Girish*, <b>Mohd Mujtaba Akhtar*</b>, Swarup Ranjan Behera, Priyabrata Mallick, Santanu Roy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.01148" target="_blank">Paper</a>
        </li>
        <li>
          <b>Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models</b><br>
          Orchid Chetia Phukan*, Girish*, <b>Mohd Mujtaba Akhtar*</b>, Swarup Ranjan Behera, Priyabrata Mallick, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">INTERSPEECH 2025</span>
          <a href="https://arxiv.org/abs/2506.03364" target="_blank">Paper</a>
        </li>
        <li>
          <b>Are Mamba-based Audio Foundation Models the best fit for non-verbal emotion recognition?</b><br>
          <b>Mohd Mujtaba Akhtar*</b>, Orchid Chetia Phukan*, Girish*, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">EUSIPCO 2025</span>
          <a href="https://arxiv.org/abs/2506.02258" target="_blank">Paper</a>
        </li>
        <li>
          <b>Source Tracing of Synthetic Speech Systems Through Paralinguistic Pre-Trained Representations</b><br>
          Girish*, <b>Mohd Mujtaba Akhtar*</b>, Orchid Chetia Phukan*, Drishti Singh*, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma<br>
          <span class="pub-venue">EUSIPCO 2025</span>
          <a href="https://arxiv.org/abs/2506.01157" target="_blank">Paper</a>
        </li>
        <li>
          <b>Strong Alone, Stronger Together: Synergizing Modality-Binding Foundation Models with Optimal Transport for Non-Verbal Emotion Recognition</b><br>
          Orchid Chetia Phukan, <b>Mohd Mujtaba Akhtar*</b>, Girish*, Swarup Ranjan Behera, Sishir Kalita, Arun Balaji Buduru, Rajesh Sharma, S.R Mahadeva Prasanna<br>
          <span class="pub-venue">ICASSP 2025</span>
        </li>
        <li>
          <b>NeuRO: an application for code-switched autism detection in children</b><br>
          <b>Mohd Mujtaba Akhtar*</b>, Girish*, Orchid Chetia Phukan*, Muskaan Singh*<br>
          <span class="pub-venue">INTERSPEECH 2024 Show & Tell</span>
          <a href="https://www.isca-archive.org/interspeech_2024/akhtar24_interspeech.html" target="_blank">Paper</a>
        </li>
      </ul>
      <h2>Journal Publications</h2>
      <ul>
        <li>
          <b>Optimizing Audio Encryption Efficiency: A Novel Framework Using Double DNA Operations and Chaotic Map-Based Techniques</b><br>
          <b>Mohd Mujtaba Akhtar</b>, Muskaan Singh, Virender Kadyan, Mohit Dua<br>
          <span class="pub-venue">Computer’s & Electrical Engineering 2025</span>
          <a href="https://ieeexplore.ieee.org/abstract/document/10889257" target="_blank">Paper</a>
        </li>
      </ul>
    </section>
    
    <section class="section spa-section" id="news" style="display:none">
      <h2>News</h2>
      
<ul class="news-list">
  <li>
  <span class="news-date">June 2025</span>
  <b>7 papers</b> accepted at <b>INTERSPEECH 2025</b> as first author.
</li>

  <li>
    <span class="news-date">June 2025</span>
    <b>2 papers</b> accepted at <b>EUSIPCO 2025</b> as first author.
  </li>
   <li>
    <span class="news-date">June 2025</span>
    <b>1 papers</b> accepted at <b>ICASSP 2025</b> as second author author.
  </li>

  <!-- <li>
    <span class="news-date">Apr 2024</span>
    Paper “Optimizing Audio Encryption Efficiency: A Novel Framework Using Double DNA Operations and Chaotic Map-Based Techniques” accepted in Computer’s & Electrical Engineering (Elsevier).
  </li> -->
  <!-- <li>
    <span class="news-date">Mar 2024</span>
    Demo paper “NeuRO: an application for code-switched autism detection in children” accepted at INTERSPEECH 2024 Show & Tell.
  </li> -->
  <li>
    <span class="news-date">Feb 2024</span>
    Submitted multiple first-author and co-authored papers to <b>INTERSPEECH, ICASSP, and EUSIPCO 2025.</b>
  </li>
<!--   <li>
  <span class="news-date">July 2024</span>
  Started <b>Master of Technology (M.Tech)</b> in <b>Computer Science</b> at Veer Bahadur Singh Purvanchal University.
</li> -->

  <li>
    <span class="news-date">Sept 2023</span>
    Completed internship as Software Developer at IBM, Gurgaon.
  </li>
  <li>
    <span class="news-date">July 2023</span>
    Received First-Class with Distinction in B.Tech (AI-ML) from UPES.
  </li>
</ul>

    </section>
  </main>
  <!-- <footer>
    &copy; 2025 Mohd Mujtaba Akhtar
  </footer> -->
  <script>
    // SPA navigation for sections
    document.querySelectorAll('.nav-link').forEach(link => {
      link.addEventListener('click', function(e) {
        e.preventDefault();
        document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
        this.classList.add('active');
        document.querySelectorAll('.spa-section').forEach(sec => sec.style.display = 'none');
        document.getElementById(this.dataset.section).style.display = '';
        window.scrollTo({top: 0, behavior: 'smooth'});
      });
    });
    // Dark mode
    document.getElementById("toggle-theme").onclick = function() {
      document.body.classList.toggle("dark");
      this.innerHTML = document.body.classList.contains("dark")
        ? '<i class="fas fa-sun"></i>'
        : '<i class="fas fa-moon"></i>';
    };
  </script>
</body>
</html>
